{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cba2bb8-1303-4fe8-8f7f-2351c6340d02",
   "metadata": {},
   "source": [
    "#  Fine-tuning a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d8efb4-ba0a-4395-a76a-28b56743c9e2",
   "metadata": {},
   "source": [
    "## Processing the data\n",
    "\n",
    "Here is a first small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41410b43-673a-4e67-bd30-b344a70a2de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peltouz/anaconda3/envs/hf/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/peltouz/anaconda3/envs/hf/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "batch[\"labels\"] = torch.tensor([1, 1]) # Set labels, here both sequence are labelled as 1\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d990d-6316-429d-a3ad-222b95e655b4",
   "metadata": {},
   "source": [
    "##  Loading a dataset from the Hub\n",
    "\n",
    "- The Hub contain models multiple datasets in lots of different languages.(https://huggingface.co/datasets)\n",
    "\n",
    "- **MRPC dataset**: This is one of the 10 datasets composing the GLUE benchmark, which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ecabbd6-15b8-4a89-984b-c7569b9303d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36baf589-dd6f-463d-9e8d-89298f7c378c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632f3eb-7827-4908-9540-a10b86eb2e48",
   "metadata": {},
   "source": [
    "- Labels are already in integer format; no preprocessing is needed.\n",
    "- To identify the integer-to-label mapping just inspect the features of the `raw_train_dataset`.\n",
    "- Integer mapping:\n",
    "   - `0` corresponds to `not_equivalent`.\n",
    "   - `1` corresponds to `equivalent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "254b8a2c-5ef8-41b2-afeb-7f0b116f51ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944da389-4381-49ab-b943-6e2976411e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\",\n",
       " 'They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][\"sentence1\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb888b28-37d6-4639-9ba8-244b0f158bbe",
   "metadata": {},
   "source": [
    "# Preprocessing a dataset\n",
    "\n",
    "To preprocess the dataset, we need to convert the text to numbers the model can make sense of. This is done with a tokenizer. We can feed the tokenizer one sentence or a list of sentences, so we can directly tokenize all the first sentences and all the second sentences of each pair like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ad3e81f-d97c-4a02-93ca-f6b2d74c0fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cade06-c210-4e7a-a0da-7664ff0e2d55",
   "metadata": {},
   "source": [
    "- A direct input of two sequences to the model won't yield a prediction for whether the sentences are paraphrases.\n",
    "- The two sequences must be handled as a pair and preprocessed appropriately.\n",
    "- The tokenizer can accept a pair of sequences and process them in the format required by the BERT model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb4b8271-15c6-49fa-a60c-0e1c2a968528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b3c47-2aa3-420e-a111-c40d5b7859fc",
   "metadata": {},
   "source": [
    "- `token_type_ids` indicates to the model the segmentation of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bc5f3ab-0476-4e6e-a54d-72ad75dd36d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'sentence',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'second',\n",
       " 'one',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c30ddae-3e33-43b7-8907-e4fbf4683e8d",
   "metadata": {},
   "source": [
    "- **Token Type IDs**:\n",
    "  - Parts of the input corresponding to `[CLS] sentence1 [SEP]` have a token type ID of 0.\n",
    "  - Parts of the input corresponding to `sentence2 [SEP]` have a token type ID of 1.\n",
    "\n",
    "- **Handling Token Type IDs**:\n",
    "  - Generally, there’s no need to worry about token_type_ids in tokenized inputs.\n",
    "  - As long as the tokenizer and model use the same checkpoint, the tokenizer will correctly provide the required information.\n",
    "\n",
    "- **Tokenizing a Dataset**:\n",
    "  - The tokenizer can handle a list of sentence pairs by taking separate lists for the first and second sentences.\n",
    "  - This approach is compatible with padding and truncation options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31ab3ca9-f831-4529-9a51-cee7613a2299",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827fc0c5-3288-4d98-9390-2a540ee7e1fe",
   "metadata": {},
   "source": [
    "- The initial method works well but has some limitations:\n",
    "  - Returns a dictionary with specific keys (`input_ids`, `attention_mask`, `token_type_ids`) and values as lists of lists.\n",
    "  - Requires sufficient RAM to store the entire dataset during tokenization.\n",
    "  - In contrast, 🤗 Datasets library datasets are Apache Arrow files, stored on disk, so only the requested samples are loaded in memory.\n",
    "\n",
    "\n",
    "- To address these limitations and retain the dataset format:\n",
    "  - Use `Dataset.map()` method.\n",
    "  - This method allows for extra preprocessing beyond tokenization.\n",
    "  - `map()` applies a function to each element in the dataset, enabling customized tokenization functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd9eee23-c417-4c3d-b142-3fbbc5dbaa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e71fb4ae-ac4b-4ab8-99d1-4421bff2f8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_function( raw_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b0dea-812f-419e-a429-fd8f3e8297d8",
   "metadata": {},
   "source": [
    "- The function takes a dictionary as input (similar to dataset items) and returns a new dictionary with the keys:\n",
    "  - `input_ids`\n",
    "  - `attention_mask`\n",
    "  - `token_type_ids`\n",
    "  \n",
    "- The function can handle multiple samples simultaneously:\n",
    "  - Each key can contain a list of sentences.\n",
    "  - This allows for using `batched=True` in the `map()` call, enhancing tokenization speed by processing multiple samples at once.\n",
    "\n",
    "\n",
    "- Padding optimization:\n",
    "  - Padding is excluded from the function to avoid inefficiency.\n",
    "  - Instead, padding is applied when building a batch, so only the maximum length in each batch is padded, not across the entire dataset.\n",
    "  - This strategy saves time and processing power, especially with variable-length inputs.\n",
    "\n",
    "\n",
    "- Application on datasets:\n",
    "  - The tokenization function is applied to all datasets at once using `batched=True` with `map()`.\n",
    "  - The `Datasets` library adds new fields to each dataset based on the keys in the returned dictionary for efficient preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d552b721-6204-4da7-bf32-42f30b1bab58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d3e1e2-8242-432c-83d3-72c2a187d55e",
   "metadata": {},
   "source": [
    "- **Tokenize Function Output**:\n",
    "  - Returns a dictionary with the following keys:\n",
    "    - `input_ids`\n",
    "    - `attention_mask`\n",
    "    - `token_type_ids`\n",
    "  - These fields are added to all splits of the dataset.\n",
    "  \n",
    "\n",
    "- **Customization with map()**:\n",
    "  - Possible to modify existing fields in the dataset by returning new values for an existing key in the preprocessing function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7cc287",
   "metadata": {},
   "source": [
    "## With your own Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4365360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'E:\\Pierre\\text1995.csv')\n",
    "data_wang = pd.read_json(r'E:\\Pierre\\Result\\wang_all\\concept_2_3_0_restricted50\\1995.json')\n",
    "data_wang['score'] = data_wang['concept_2_wang_3_restricted50'].apply(lambda x: x['score']['novelty'] )\n",
    "df = pd.merge(df[['id','text']],data_wang[['id','score']],on = 'id', how = 'inner')\n",
    "\n",
    "\n",
    "num_positive = df[df['score'] > 0].shape[0]\n",
    "zero_score_subset = df[df['score'] == 0].sample(n=num_positive, random_state=42) \n",
    "balanced_df = pd.concat([df[df['score'] == 1], zero_score_subset])\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=24).reset_index(drop=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "balanced_df = balanced_df[['id','score', 'text']].set_index('id')\n",
    "train_df, temp_df = train_test_split(balanced_df, test_size=0.4, random_state=42)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42) \n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': valid_dataset,\n",
    "    'test':test_dataset\n",
    "})\n",
    "datasets.save_to_disk('test_novelty')\n",
    "\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "datasets = load_from_disk('test_novelty')\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir='hub')\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"id\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"score\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea4c9e-89c6-4d5b-a727-ff4932076a9d",
   "metadata": {},
   "source": [
    "## Dynamic padding\n",
    "\n",
    "- **Collate Function**: \n",
    "  - The collate function organizes samples within a batch in a DataLoader.\n",
    "  - Default behavior: Converts samples to PyTorch tensors and concatenates them, handling lists, tuples, or dictionaries recursively.\n",
    "  - Limitation: This approach won’t work if inputs vary in size.\n",
    "\n",
    "\n",
    "- **Batch Padding Strategy**:\n",
    "  - Padding is deliberately applied only as needed for each batch to minimize excessive padding.\n",
    "  - Benefits: Speeds up training by reducing over-long inputs.\n",
    "\n",
    "\n",
    "\n",
    "- **Custom Collate Function with Padding**:\n",
    "  - A custom collate function applies appropriate padding to batch items.\n",
    "  - Transformers library provides `DataCollatorWithPadding` for this purpose.\n",
    "    - Requires a tokenizer to handle padding tokens and specify left or right padding as needed by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f3ddbba-7b07-4cd0-80dc-8e64b264ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eb7071b-e0ed-44eb-bb05-dd74dcb0aa73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a80491-ee9a-4736-8d24-f1be9ea571e1",
   "metadata": {},
   "source": [
    "- Samples have varying lengths, ranging from 32 to 67.\n",
    "- **Dynamic padding**: Pads samples in a batch to the maximum length within that batch (67 in this case).\n",
    "- **Without dynamic padding**: Would require padding all samples to the maximum length across the entire dataset or to the model's maximum acceptable length.\n",
    "- A check on `data_collator` confirms proper application of dynamic padding for the batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91408409-e7be-45bc-9644-727df191b34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ca4011-334a-4a1e-842d-d1fc2cea3d4d",
   "metadata": {},
   "source": [
    "# Fine-tuning a model with the Trainer API\n",
    "\n",
    "- **Trainer Class in Transformers**:\n",
    "  - The Trainer class is provided by Transformers for fine-tuning pretrained models on custom datasets.\n",
    "  - After data preprocessing, only a few steps are needed to define the Trainer.\n",
    "\n",
    "\n",
    "- **Setting Up the Training Environment**:\n",
    "  - Running `Trainer.train()` on a CPU is very slow; a GPU is recommended.\n",
    "  - Google Colab offers access to free GPUs and TPUs for faster training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfd41eee-168e-48ef-82db-adaaa75f58c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996185fd-c19f-4759-a69d-0d81e691d77f",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- **Define TrainingArguments**: \n",
    "  - Before defining the Trainer, set up a `TrainingArguments` class.\n",
    "  - This class will include all the necessary hyperparameters for training and evaluation.\n",
    "\n",
    "\n",
    "- **Required Argument**:\n",
    "  - Specify a directory where:\n",
    "    - The trained model will be saved.\n",
    "    - Checkpoints will be stored during training.\n",
    "\n",
    "\n",
    "- **Default Settings**:\n",
    "  - Defaults for other parameters are generally sufficient for basic fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5952d706-8101-4d15-9426-b1a8f3470930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36ed9b5e-99ac-44d6-8889-fc0a48a36b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf0d7e-2353-4329-b184-f50c9b4f611c",
   "metadata": {},
   "source": [
    "- **Warning on Model Instantiation**:\n",
    "  - A warning appears after loading the pretrained BERT model.\n",
    "  - BERT was not pretrained for sentence pair classification, so the original model head is discarded.\n",
    "  - A new head for sequence classification is added, causing:\n",
    "    - Some weights to be unused (from the discarded pretraining head).\n",
    "    - Some weights to be randomly initialized (for the new classification head).\n",
    "  - The warning suggests training the model to optimize the new head.\n",
    "\n",
    "- **Defining a Trainer**:\n",
    "  - The Trainer requires the following components:\n",
    "    - The modified model (with a new head).\n",
    "    - `training_args`: settings and configurations for the training process.\n",
    "    - `training` and `validation` datasets.\n",
    "    - `data_collator`: a function to collate batches of data.\n",
    "    - `tokenizer`: to process text inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b20d293d-15e0-4c37-b270-5d47dadbc377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1f891-8072-444c-a606-02796349f0d0",
   "metadata": {},
   "source": [
    " - To fine-tune the model on our dataset, we just have to call the train() method of our Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4987f-801e-4bfc-829f-467ca03da699",
   "metadata": {},
   "source": [
    "- The fine-tuning process will begin, which should take a few minutes on a GPU.\n",
    "- Training loss will be reported every 500 steps.\n",
    "- However, model performance (quality) is not assessed due to:\n",
    "  - Lack of evaluation strategy:\n",
    "    - `evaluation_strategy` was not set to \"steps\" (evaluate every `eval_steps`) or \"epoch\" (evaluate at the end of each epoch).\n",
    "  - Absence of `compute_metrics()` function:\n",
    "    - Without this, no metrics are calculated during evaluation; only the loss would be printed, which is not very informative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea144db-436d-45a8-995c-c1a320f8fc14",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "- **Goal**: Build a `compute_metrics()` function to use during model training.\n",
    "\n",
    "- **Function Requirements**:\n",
    "  - Accepts an `EvalPrediction` object (a named tuple with:\n",
    "    - `predictions` field\n",
    "    - `label_ids` field)\n",
    "  - Returns a dictionary:\n",
    "    - Keys are metric names (strings)\n",
    "    - Values are metric values (floats)\n",
    "\n",
    "- **Usage**:\n",
    "  - Use `Trainer.predict()` to generate model predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca6848e9-f3d2-47c2-9a21-47a4a48895d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20fdc39-c1f4-4130-ab3b-5700238fde7a",
   "metadata": {},
   "source": [
    "- **predict() method output**:\n",
    "  - Returns a named tuple with three fields:\n",
    "    - **predictions**: \n",
    "      - A 2D array with shape 408 x 2 (for 408 elements in the dataset).\n",
    "      - Contains logits for each element, which need to be transformed to make predictions.\n",
    "      - Transformation process: select the index with the maximum value on the second axis.\n",
    "    - **label_ids**: Stores the labels for comparison.\n",
    "    - **metrics**:\n",
    "      - Initially includes:\n",
    "        - **Loss** on the dataset passed.\n",
    "        - **Time metrics** (total and average prediction time).\n",
    "      - When `compute_metrics()` is defined and passed to `Trainer`, `metrics` also includes the metrics returned by `compute_metrics()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51d04449-1973-4794-ac4f-7589e209dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518df025-f0ff-4f39-917f-623cafe1ab51",
   "metadata": {},
   "source": [
    " - We can now compare those preds to the labels.\n",
    " - To build our compute_metric() function, we will rely on the metrics from the Evaluate library. \n",
    " - We can load the metrics associated with the MRPC dataset as easily as we loaded the dataset, this time with the evaluate.load() function.\n",
    " - The object returned has a compute() method we can use to do the metric calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecac16cc-4cd5-4d9f-b854-5c3922c43b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3d3ccae-e4ee-481d-a5a5-50c1cbbed398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9364ed87-f92d-498d-b3ee-307c28fde656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peltouz/anaconda3/envs/hf/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f75482-6c34-4683-90cc-570d5829ec3f",
   "metadata": {},
   "source": [
    "- **New TrainingArguments**:\n",
    "  - A new `TrainingArguments` object is created.\n",
    "  - The `evaluation_strategy` parameter is set to `\"epoch\"`.\n",
    "\n",
    "\n",
    "- **New Model**:\n",
    "  - A new model is instantiated for training.\n",
    "  - This prevents continuing training on an already trained model.\n",
    "\n",
    "\n",
    "- To launch a new training run, we execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c278debb-90ba-4da8-a1c6-3d4dc14101e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 25:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.328425</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.911612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.504700</td>\n",
       "      <td>0.570676</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.894366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.662599</td>\n",
       "      <td>0.867647</td>\n",
       "      <td>0.906574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3190316066589577, metrics={'train_runtime': 1551.2139, 'train_samples_per_second': 7.094, 'train_steps_per_second': 0.888, 'total_flos': 405114969714960.0, 'train_loss': 0.3190316066589577, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913191c-fc77-415a-870b-51f20a77a9aa",
   "metadata": {},
   "source": [
    "- The model will now:\n",
    "  - Report validation loss and metrics (accuracy, F1 score) at the end of each epoch.\n",
    "  - Continue reporting training loss.\n",
    "\n",
    "- Note:\n",
    "  - The exact accuracy/F1 score may vary slightly due to the model's random head initialization.\n",
    "  - Despite this variability, results should remain close to the expected range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d54022a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_pred = trainer.predict(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd29917a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8353623188405798, 'f1': 0.8807724601175483}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.argmax(test_pred.predictions, axis=-1)\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=test_pred.label_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HuggingFace",
   "language": "python",
   "name": "hs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
