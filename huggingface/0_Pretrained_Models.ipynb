{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feffe63f-bee0-4dc1-a237-b147a79933fa",
   "metadata": {},
   "source": [
    "# Transformers model\n",
    "\n",
    "## For which tasks?\n",
    "\n",
    "- **Classifying whole sentences**:\n",
    "  - Examples: Sentiment analysis, spam detection, grammatical correctness, sentence relationship.\n",
    "\n",
    "\n",
    "- **Classifying each word in a sentence**:\n",
    "  - Examples: Grammatical components (noun, verb, adjective), named entity recognition (person, location, organization).\n",
    "\n",
    "\n",
    "- **Generating text content**:\n",
    "  - Examples: Autocomplete text from a prompt, fill in masked words in a text.\n",
    "\n",
    "\n",
    "- **Extracting an answer from a text**:\n",
    "  - Examples: Given a question and context, extract the answer based on the context.\n",
    "\n",
    "\n",
    "- **Generating a new sentence from an input text**:\n",
    "  - Examples: Translation, text summarization.\n",
    "\n",
    "\n",
    "### A challenging task\n",
    "\n",
    "- **Human vs Machine Language Processing**:\n",
    "  - Humans understand sentences like \"I am hungry\" easily, and can compare similar sentences like \"I am hungry\" and \"I am sad\".\n",
    "  - Machine learning models find it more difficult to process and understand language, requiring careful text processing.\n",
    "\n",
    "\n",
    "- **Introduction of Transformers**:\n",
    "  - **2017**: Introduction of Transformers by Vaswani et al. in the paper \"Attention Is All You Need.\"\n",
    "    - Neural network architecture learning context and relationships from sequential data, initially focused on translation tasks.\n",
    "\n",
    "\n",
    "- **Influential Transformer Models**:\n",
    "  - **June 2018**: GPT - First pretrained transformer model, state-of-the-art results on various NLP tasks.\n",
    "  - **October 2018**: BERT - Designed to produce better sentence summaries.\n",
    "  - **February 2019**: GPT-2 - Improved version of GPT, not immediately released due to ethical concerns.\n",
    "  - **October 2019**: DistilBERT - A distilled version of BERT, 60% faster, 40% lighter, retaining 97% of BERTâ€™s performance.\n",
    "  - **October 2019**: BART and T5 - Pretrained models using the original Transformer architecture.\n",
    "  - **May 2020**: GPT-3 - Larger model, capable of zero-shot learning.\n",
    "  - **November 2022**: GPT-3.5 and **March 2023**: GPT-4.\n",
    "  - Followed by other models like **Llama**, **Claude**, **Gemini**, **Mistral**.\n",
    "\n",
    "\n",
    "\n",
    "# Pretraining and Fine-tuning\n",
    "\n",
    "- **Transformer models (GPT, BERT, BART, T5, etc.)** are trained as language models on large amounts of raw text using self-supervised learning.\n",
    "   - Self-supervised learning: the model learns without human-labeled data, as the objective is computed automatically from the inputs.\n",
    "   - These models develop a statistical understanding of language but aren't directly useful for specific tasks.\n",
    "\n",
    "- **Transfer learning**: the process where a pretrained model is fine-tuned on specific tasks using supervised learning (human-annotated labels).\n",
    "   - Pretrained models undergo fine-tuning to adapt to particular tasks.\n",
    "\n",
    "\n",
    "- **Pretraining**:\n",
    "   - The model is trained from scratch with randomly initialized weights.\n",
    "   - Pretraining is resource-intensive in terms of time, data, and money.\n",
    "   - It requires a large corpus and can take weeks to complete.\n",
    "\n",
    "\n",
    "- **Fine-tuning**:\n",
    "   - Performed after pretraining using a dataset specific to a task.\n",
    "   - Reasons to fine-tune instead of training from scratch:\n",
    "     - The pretrained model shares similarities with the fine-tuning dataset.\n",
    "     - Less data and time are required to achieve good results.\n",
    "     - Lower costs in terms of time, data, financial, and environmental resources.\n",
    "     - It allows for faster iteration and refinement.\n",
    "\n",
    "\n",
    "- **Transfer learning advantages**:\n",
    "   - Leverages knowledge from pretraining for improved task-specific performance.\n",
    "   - Fine-tuning achieves better results than training from scratch unless massive data is available.\n",
    "   - Using pretrained models close to the target task optimizes performance.\n",
    "\n",
    "\n",
    "\n",
    "# Transformers Library\n",
    "\n",
    "- The ðŸ¤— Transformers library provides tools to create and use shared models.\n",
    "\n",
    "\n",
    "- **Model Hub**:\n",
    "  - Contains thousands of pretrained models for download and use.\n",
    "  - Users can also upload their own models to the Hub.\n",
    "  \n",
    "  \n",
    "- **Pipeline() function**:\n",
    "  - The most basic object in the library.\n",
    "  - Connects a model with necessary preprocessing and postprocessing steps.\n",
    "  - Allows direct input of text for intelligible output.\n",
    "\n",
    "\n",
    "- **Three main steps in a pipeline**:\n",
    "  1. Text is preprocessed into a format the model understands.\n",
    "  2. Preprocessed inputs are passed to the model.\n",
    "  3. Model predictions are post-processed for easy interpretation.\n",
    "\n",
    "\n",
    "- **Available pipelines**:\n",
    "  - feature-extraction (vector representation of a text)\n",
    "  - fill-mask\n",
    "  - ner (named entity recognition)\n",
    "  - question-answering\n",
    "  - sentiment-analysis\n",
    "  - summarization\n",
    "  - text-generation\n",
    "  - translation\n",
    "  - zero-shot-classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86dea64",
   "metadata": {},
   "source": [
    "## Setting the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85edc63a-7cfe-450e-954e-b4f270415ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "hf_token = \"your_token\"\n",
    "custom_cache_dir = '/home/peltouz/Documents/pretrain'\n",
    "\n",
    "os.environ['HF_HOME'] = custom_cache_dir  # Hugging Face home directory for all HF operations\n",
    "os.environ['TRANSFORMERS_CACHE'] = custom_cache_dir  # Transformers-specific cache directory\n",
    "os.environ['HF_DATASETS_CACHE'] = custom_cache_dir  # Datasets-specific cache directory\n",
    "os.environ['HF_METRICS_CACHE'] = custom_cache_dir  # Metrics-specific cache directory\n",
    "os.environ['HF_TOKEN'] = hf_token  # Hugging Face API token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04981977-725c-47af-a555-a09f59ceec9e",
   "metadata": {},
   "source": [
    "This Python code snippet configures the environment to specify custom cache directories for operations involving Hugging Face libraries, and it sets an API token for authentication.\n",
    "\n",
    "- **API Token Configuration**:\n",
    "   - `hf_token`: Assigned to a string representing the Hugging Face API token.\n",
    "   - Purpose: Used for authenticating and accessing Hugging Face services that require credentials.\n",
    "\n",
    "\n",
    "- **Custom Cache Directory**:\n",
    "   - `custom_cache_dir`: Set to `D:/pretrain`.\n",
    "   - Purpose: Specifies a base directory for storing cache files related to Hugging Face operations.\n",
    "\n",
    "\n",
    "- **Environment Variables**:\n",
    "   - `HF_HOME`: Specifies the base directory for all Hugging Face-related operations.\n",
    "   - `TRANSFORMERS_CACHE`: Directory for caching models downloaded via the transformers library.\n",
    "   - `HF_DATASETS_CACHE`: Directory for caching datasets accessed via the datasets library.\n",
    "   - `HF_METRICS_CACHE`: Directory for caching metrics-related files used in model evaluation.\n",
    "   - `HF_TOKEN`: Environment variable for the Hugging Face API token to authenticate requests to Hugging Face services.\n",
    "\n",
    "\n",
    "- **Purpose of Environment Variables**:\n",
    "   - Ensure all data related to models, datasets, and metrics are stored in the specified directory (`D:/pretrain`).\n",
    "   - Help manage disk space, especially when handling large models or datasets.\n",
    "   - Correctly configure credentials for accessing restricted services.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de3e29",
   "metadata": {},
   "source": [
    "# The Pipeline function\n",
    "\n",
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d73a5746-323b-4646-8fd5-082f19ec0580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65eb750a27524abda14e13de076859ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bc68d745e94a959ba724db1738b559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10f3ea304bd4721a710d1f76d49391c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d10bf641bc043b3baef52a619ae7dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9989008903503418}, {'label': 'POSITIVE', 'score': 0.9998173117637634}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NEGATIVE'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "sentiments = classifier(\n",
    "        [\"I hate teaching\",\n",
    "         \"I love programming\"]\n",
    "    )\n",
    "\n",
    "print(sentiments)\n",
    "sentiments[0]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa9a7f5-901d-43b7-9905-3ef917b6edde",
   "metadata": {},
   "source": [
    "  - A pretrained model fine-tuned for sentiment analysis in English is selected by default.\n",
    "  - The model is downloaded and cached when the classifier object is created.\n",
    "  - Upon rerunning the command, the cached model is used without needing to download again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da85f3b",
   "metadata": {},
   "source": [
    "\n",
    "## Zero-shot classification\n",
    "\n",
    " - **Main idea**: \n",
    "   - Classifying unlabelled texts is a challenging task often encountered in real-world projects.\n",
    "\n",
    "\n",
    "- **Comparison**: \n",
    "  - Annotating text manually is time-consuming and requires domain expertise.\n",
    "\n",
    "\n",
    "- **Key aspect**: \n",
    "  - The zero-shot-classification pipeline is powerful because it lets you specify your own labels for classification, instead of relying on pretrained model labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86875fb9-b63f-4b08-9441-ecef7316505a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli (https://huggingface.co/facebook/bart-large-mnli)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3948687598aa4adf83c62a3582eb5a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef62bb411f8d4a81870dbdca228cfb95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec7d43e23144042831567fb60472d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab4a2354fe24f39bd85776db71a8321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6e7b922558491d9a66a13e0234323f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832bf23daea1410ab6286f02056242cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'df %>% filter(!is.na(var1))',\n",
       " 'labels': ['Rstudio', 'python'],\n",
       " 'scores': [0.6543465852737427, 0.3456534445285797]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"df %>% filter(!is.na(var1))\",\n",
    "    candidate_labels=[\"python\", \"Rstudio\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2c48af-9c76-430a-b592-28ef61f55264",
   "metadata": {},
   "source": [
    "## Text generation\n",
    "\n",
    "- **Main idea**: \n",
    "  - A prompt is provided, and the model auto-completes it by generating the remaining text.\n",
    "  \n",
    "  \n",
    "- **Comparison**: \n",
    "  - Similar to the predictive text feature on phones.\n",
    "  \n",
    "  \n",
    "- **Key aspect**: \n",
    "  - Text generation involves randomness, so results may vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311eeb43-6587-43aa-a5f5-bd63c3fdefab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910117f3209c424f90b7fef9464b9b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7e5f53d7384f12bae4333b56c199cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/523M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5867ed0a3674b32966fc00a4366458f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a24f1f5698047a7a882a97193bf0696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75540a1a8c7f4813bb4974b46a0fa6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6845e7ccc10a43a592e7891a770b5024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In my programming course in DS2E I will have some time to show some ideas of the way C++ interfaces and interfaces are implemented in C++ code. I can help many others who are developing C++ programs as well to figure out how to'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In my programming course in DS2E I will\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9779e9e7-c32d-46d0-bc65-0153c1f9cf06",
   "metadata": {},
   "source": [
    "## Mask filling\n",
    "\n",
    "- **Main idea**: \n",
    "  - The task involves filling in the blanks in a given text.\n",
    "\n",
    "\n",
    "- **Comparison**: \n",
    "  - Similar to completing sentences in cloze tests or gap-filling exercises.\n",
    "  \n",
    "\n",
    "- **Key aspect**: \n",
    "  - The focus is on providing contextually appropriate words or phrases to complete the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d91d2e48-00d5-48f6-9811-64e06474fa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c210b22cea394ba2b4a8baa8d24b9770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695c030ea5d042d38aaab98f29dc81ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/316M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d150eb37dbf4a208e025a6f9b7e6942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6724180b0b9740f7ab5f0503001628b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7ae8449ce543c38b1ff1c0dc88b41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321592e8bae743e381f3ce1a8f9bb9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.196198508143425,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models.'},\n",
       " {'score': 0.040527332574129105,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This course will teach you all about computational models.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8025baa0-3176-4718-9a1a-95173b265574",
   "metadata": {},
   "source": [
    "## Named entity recognition\n",
    "\n",
    "- **Main idea**: \n",
    "  - Named entity recognition (NER) involves identifying parts of the input text that correspond to entities.\n",
    "\n",
    "\n",
    "- **Comparison**: \n",
    "  - NER focuses on entities like persons, locations, or organizations in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5ada528-69da-40d1-989d-719b1f9bf6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80929e45b91846a88b91e4bec04766d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e342c70dd34188958fa2a2d8ad12d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438efbc872b84a088ef7f34f73b109ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6746dbde1b8245f481f213d132e54429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peltouz/.local/lib/python3.6/site-packages/transformers/pipelines/token_classification.py:136: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  f'`grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"{aggregation_strategy}\"` instead.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99918455,\n",
       "  'word': 'Pierre',\n",
       "  'start': 11,\n",
       "  'end': 17},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9977419,\n",
       "  'word': 'BETA',\n",
       "  'start': 32,\n",
       "  'end': 36},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9894749,\n",
       "  'word': 'Strasbourg',\n",
       "  'start': 40,\n",
       "  'end': 50}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Pierre and I work at BETA in Strasbourg.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5744d73a-a5da-4a13-b77c-48e08c5e6aa3",
   "metadata": {},
   "source": [
    "## Question answering\n",
    "\n",
    "The question-answering pipeline answers questions using information from a given context:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a30660e-403e-4b4b-a991-501d54bf76e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0852b916921048188d5d68f41bb1f1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd696dff1d8747c3b3efa526397395ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c7c7dc4e204b13aa592a701ebdf996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e08c27a2ee3491f89aa488d2f7f392d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525e787784f44abdafe6b130b31ebf22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.5030694603919983, 'start': 32, 'end': 36, 'answer': 'BETA'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Pierre and I work at BETA in Strasbourg.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c53cd1-6559-4f05-838e-24fe32869120",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text. Hereâ€™s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c6efd4c-dac1-42e1-9c27-af1b4ea795cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aaaf68314024f6d82a0a23b01bf293e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8415784d5bb549ad92f2e9cc085dfa0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8123cbd3a07e45e283552af29a35413a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbaf3af62364a588e945efe87a7abb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea358b0668194cf98444c10e8734ce04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Neural network-based technology meets the essential properties of emerging technologies in the scientific realm . It is novel, because it shows discontinuous innovations in the originating domain and is put to new uses in many application domains . Researchers propose a new conceptual framework that considers artificial intelligence as an emerging general method of invention .'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\"\"\"This paper offers insights into the diffusion and impact of artificial intelligence in science.\n",
    "More specifically, we show that neural network-based technology meets the essential properties of emerging technologies in the scientific realm.\n",
    "It is novel, because it shows discontinuous innovations in the originating domain and is put to new uses in many application domains;\n",
    "it is quick growing, its dimensions being subject to rapid change; it is coherent, because it detaches from its technological parents, and integrates and is accepted in different scientific communities;\n",
    "and it has a prominent impact on scientific discovery, but a high degree of uncertainty and ambiguity associated with this impact.\n",
    "Our findings suggest that intelligent machines diffuse in the sciences, reshape the nature of the discovery process and affect the organization of science.\n",
    "We propose a new conceptual framework that considers artificial intelligence as an emerging general method of invention and, on this basis, derive its policy implications.\n",
    "\"\"\"\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bc8552-171d-4bb9-ba7a-3ffaa030b062",
   "metadata": {},
   "source": [
    "# Using specific model\n",
    "\n",
    "See this **Warning message**: \"Using a pipeline without specifying a model name and revision in production is not recommended.\"\n",
    "  \n",
    "  \n",
    "- **Recommendation**:\n",
    "  - Specify the model name and revision when using pipelines in production environments.\n",
    "\n",
    "\n",
    "- **Actionable step**:\n",
    "  - Choose a specific model from the 1M+ models available on Hugging Face.\n",
    "\n",
    "\n",
    "- **Resource**:\n",
    "  - Hugging Face model repository: [https://huggingface.co/models](https://huggingface.co/models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c0fdb46-3699-4fff-a0a0-8f8a174e39bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37006f206fd4bc883280febac8e8df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c53f2d92ce94506bd20fac49315f05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/336M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ef54dc9cd74c2eb5846fa2430d3ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac81633caf0b494cb58249d8d677bc16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb03e4fcd6a4120a52c253be2d22b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdecf1c34ae4000a2a315f4a7b68b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In my programming course in DS2E I will tell you that I know of how I build it.\\n\\nThe way in which I build it'},\n",
       " {'generated_text': 'In my programming course in DS2E I will use a lot of Python to program the program. I will also have to make sure any code that'},\n",
       " {'generated_text': 'In my programming course in DS2E I will explore an interesting and interesting subject.\\nThe first stage is the evolution or evolution of the DS technology'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In my programming course in DS2E I will\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3540dbc0-b013-41c0-a18a-818deb8162ca",
   "metadata": {},
   "source": [
    "## Translation\n",
    "\n",
    "For translation, you can use a default model if you provide a language pair in the task name (such as \"translation_en_to_fr\"), but the easiest way is to pick the model you want to use on the Model Hub. \n",
    "\n",
    "Here weâ€™ll try translating from French to English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc110bfb-9aa1-4364-a65f-96d324ca16bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62053f829a24278b5cdc9f60a389f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314d7d86bef546f298e6b9c3f034349c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a644571a86eb4f48b14cc180a5aa03eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe952682e8594944a47fc4153afe3af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/784k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19e6b5c52e44507ae099be63c21eff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5b0484685241e3b04e763657e9eb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.28M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This course is produced by Hugging Face.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e5aeb7-13b8-4364-b5e8-1c0b2b1b9a56",
   "metadata": {},
   "source": [
    "# The Transformer Archictecture \n",
    "\n",
    "- **Model Composition**:\n",
    "  - **Encoder**:\n",
    "    - Receives input and builds its representation (features).\n",
    "    - Optimized for understanding the input.\n",
    "  - **Decoder**:\n",
    "    - Uses the encoder's representation (features) and other inputs to generate a target sequence.\n",
    "    - Optimized for generating outputs.\n",
    "\n",
    "\n",
    "- **Usage**:\n",
    "  - **Encoder-only models**:\n",
    "    - Suitable for tasks requiring input understanding (e.g., sentence classification, named entity recognition).\n",
    "  - **Decoder-only models**:\n",
    "    - Suitable for generative tasks (e.g., text generation).\n",
    "  - **Encoder-decoder models (sequence-to-sequence models)**:\n",
    "    - Suitable for generative tasks that require an input (e.g., translation, summarization).\n",
    "\n",
    "\n",
    "\n",
    "## Attention layers\n",
    "\n",
    "Attention layers are integral to the Transformer architecture. The paper introducing the Transformer was titled â€œAttention Is All You Need,â€ highlighting the importance of attention layers.\n",
    "\n",
    "- **Function of attention layers**: These layers direct the model to focus on specific words in a sentence, while downplaying the importance of others.\n",
    "\n",
    "- **Contextual meaning**: The meaning of a word depends not only on the word itself but also on its context, which includes other words around it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31f0d8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### The original architecture\n",
    "\n",
    "The Transformer architecture was initially designed for translation.\n",
    "\n",
    "- **Encoder**:\n",
    "  - Receives inputs (sentences) in a certain language during training.\n",
    "  - Attention layers can use all the words in a sentence, considering both preceding and following words.\n",
    "\n",
    "- **Decoder**:\n",
    "  - Receives the same sentences in the desired target language.\n",
    "  - Works sequentially, paying attention only to the words that have already been translated (i.e., the preceding words).\n",
    "  - For instance, after predicting the first three words of the translated target, the decoder uses these words and the inputs from the encoder to predict the fourth word.\n",
    "\n",
    "![](en_chapter1_transformers.svg)\n",
    "\n",
    "- **Initial Embedding Lookup**:\n",
    "  - The raw embeddings for each token are **context-independent**.\n",
    "  - Example: The same embedding is used for \"bank\" whether it refers to a financial institution or a riverbank.\n",
    "\n",
    "\n",
    "- **Transformer Layers**:\n",
    "  - After the initial embedding lookup, token embeddings (e.g., **768-dimensional vectors**) are passed through the **transformer's self-attention layers**.\n",
    "  - These layers enable the model to attend to other tokens in the sequence, capturing relationships and interactions between words.\n",
    "\n",
    "\n",
    "- **Context-Sensitive Representations**:\n",
    "  - As the token embeddings pass through multiple transformer layers, each token's representation becomes **context-sensitive** based on surrounding words.\n",
    "\n",
    "\n",
    "## Architectures vs. checkpoints\n",
    "\n",
    "- **Architecture**: \n",
    "  - Refers to the skeleton of the model, defining each layer and operation within it.\n",
    "  \n",
    "  \n",
    "- **Checkpoints**: \n",
    "  - Weights that are loaded into a given architecture.\n",
    "\n",
    "\n",
    "- **Model**: \n",
    "  - An umbrella term that can refer to both architecture and checkpoints.\n",
    "  \n",
    "\n",
    "- **Example**: \n",
    "  - BERT is an architecture.\n",
    "  - bert-base-cased, a set of weights trained by the Google team for the first release of BERT, is a checkpoint.\n",
    "  - The term \"model\" can be used to refer to both the architecture (e.g., \"BERT model\") and the checkpoint (e.g., \"bert-base-cased model\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9d977-2d20-4cd6-80ec-968d2532f30a",
   "metadata": {},
   "source": [
    "# Encoder models\n",
    "\n",
    "- **Encoder models** use only the encoder part of a Transformer model.\n",
    "\n",
    "\n",
    "- **Attention mechanism**: At each stage, the attention layers can access all the words in the sentence.\n",
    "\n",
    "\n",
    "- These models are characterized by **bi-directional attention** and are often referred to as **auto-encoding models**.\n",
    "\n",
    "\n",
    "- **Pretraining**: Typically involves corrupting a sentence (e.g., by masking random words) and tasking the model with reconstructing the original sentence.\n",
    "\n",
    "\n",
    "- **Best suited for**: Tasks requiring a full understanding of the sentence, such as:\n",
    "    - Sentence classification\n",
    "    - Named entity recognition (word classification)\n",
    "    - Extractive question answering\n",
    "\n",
    "\n",
    "- **Examples** of encoder models:\n",
    "    - ALBERT\n",
    "    - BERT\n",
    "    - DistilBERT\n",
    "    - ELECTRA\n",
    "    - RoBERTa\n",
    "\n",
    "\n",
    "# Decoder models\n",
    "\n",
    "- **Decoder models** use only the decoder part of a Transformer model.\n",
    "\n",
    "\n",
    "- **Attention mechanism**: At each stage, the attention layers can only access the words that are positioned before the current word in the sentence.\n",
    "\n",
    "\n",
    "- These models are often referred to as **auto-regressive models**.\n",
    "\n",
    "\n",
    "- **Pretraining**: Typically focuses on predicting the next word in a sentence.\n",
    "\n",
    "\n",
    "- **Best suited for**: Tasks involving text generation.\n",
    "\n",
    "\n",
    "- **Examples** of decoder models:\n",
    "    - CTRL\n",
    "    - GPT\n",
    "    - GPT-2\n",
    "    - Transformer XL\n",
    "\n",
    "# Sequence-to-sequence models\n",
    "\n",
    "- **Encoder-decoder models** (also known as **sequence-to-sequence models**) use both parts of the Transformer architecture.\n",
    "\n",
    "\n",
    "- **Attention mechanism**:\n",
    "    - Encoder: The attention layers can access all the words in the input sentence.\n",
    "    - Decoder: The attention layers can only access the words positioned before the current word in the input.\n",
    "\n",
    "\n",
    "- **Best suited for**: Tasks that involve generating new sentences based on a given input, such as:\n",
    "    - Summarization\n",
    "    - Translation\n",
    "    - Generative question answering\n",
    "\n",
    "\n",
    "- **Examples** of encoder-decoder models:\n",
    "    - BART\n",
    "    - mBART\n",
    "    - Marian\n",
    "    - T5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b38239-f6e2-470b-9ec9-14654f3cfa6b",
   "metadata": {},
   "source": [
    "# Bias and limitations\n",
    "\n",
    "- Pretrained and fine-tuned models are powerful tools, but they have limitations.\n",
    "\n",
    "\n",
    "- The main limitation stems from the nature of pretraining on large datasets.\n",
    "  - Data is often scraped indiscriminately from the internet.\n",
    "  - This includes both high-quality and low-quality content.\n",
    "\n",
    "\n",
    "- An example is provided with a fill-mask pipeline using the BERT model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d96a13e-ff4f-480e-a101-7c6d6c4479fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e4f58-9e55-4ec4-96fe-056b2a6689a1",
   "metadata": {},
   "source": [
    "- The model provides only one gender-neutral option (waiter/waitress) for the sentence completion task.\n",
    "\n",
    "\n",
    "- Most answers involve work occupations associated with a specific gender.\n",
    "\n",
    "\n",
    "- The model's top associations for â€œwomanâ€ and â€œworkâ€ included â€œprostitute.â€\n",
    "\n",
    "\n",
    "- This result occurs despite BERT being trained on neutral datasets (English Wikipedia and BookCorpus), avoiding data scraped from the internet.\n",
    "\n",
    "\n",
    "- Users should be aware that models, even when fine-tuned, can still produce biased (sexist, racist, homophobic) content due to intrinsic biases in the original model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c4001c-9cd8-4850-bf76-264e8b07cdda",
   "metadata": {},
   "source": [
    "# Behind the pipeline function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e753cca-49a0-4868-98f6-07e2e03e0e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9989008903503418},\n",
       " {'label': 'POSITIVE', 'score': 0.9998173117637634}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "sentiments = classifier(\n",
    "        [\"I hate teaching\",\n",
    "         \"I love programming\"]\n",
    "    )\n",
    "sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccba7be-938f-4cc7-92a5-7136c6077851",
   "metadata": {},
   "source": [
    "this `pipeline` groups together three steps: preprocessing, passing the inputs through the model, and postprocessing:\n",
    "\n",
    "## Preprocessing with a tokenizer\n",
    "- **Transformer models** can't process raw text directly.\n",
    "\n",
    "\n",
    "- The first step is to **convert text inputs into numbers** using a tokenizer.\n",
    "\n",
    "\n",
    "- The tokenizer is responsible for:\n",
    "  - **Splitting the input** into tokens (words, subwords, or symbols like punctuation).\n",
    "  - **Mapping each token to an integer**.\n",
    "  - **Adding additional inputs** useful to the model.\n",
    "\n",
    "\n",
    "- Preprocessing must be consistent with how the model was pretrained.\n",
    "\n",
    "\n",
    "- **AutoTokenizer** class and its **from_pretrained()** method help fetch and cache the tokenizer information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b08f9bc8-c6ea-4c29-8593-05ade236d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48020111",
   "metadata": {},
   "source": [
    "- The default checkpoint for the **sentiment-analysis pipeline** is **distilbert-base-uncased-finetuned-sst-2-english**.\n",
    "\n",
    "\n",
    "- ðŸ¤— Transformers can be used without concern for the underlying ML framework (PyTorch, TensorFlow, or Flax).\n",
    "\n",
    "\n",
    "- Transformer models require **tensors** as input.\n",
    "\n",
    "\n",
    "- Tensors are similar to NumPy arrays, which can have:\n",
    "  - 0D (scalar),\n",
    "  - 1D (vector),\n",
    "  - 2D (matrix),\n",
    "  - or more dimensions.\n",
    "\n",
    "\n",
    "- Other ML frameworks' tensors behave similarly to NumPy arrays and are easy to instantiate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4aa7796-fe2e-49a0-8cf2-597ebe7fd524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 5223, 4252,  102],\n",
       "        [ 101, 1045, 2293, 4730,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I hate teaching\",\n",
    "    \"I love programming\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\") # Here pytorch\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29539c98-b532-4038-bf73-dd1057e99bd8",
   "metadata": {},
   "source": [
    "- **Output Structure**:\n",
    "  - A dictionary containing two keys: `input_ids` and `attention_mask`.\n",
    "  - `input_ids`: Two rows of integers, one for each sentence, representing the unique identifiers of the tokens.\n",
    "  - `attention_mask`: A tensor with the same shape as the `input_ids`, filled with 0s and 1s, where:\n",
    "    - 1s indicate tokens to be attended to.\n",
    "    - 0s indicate tokens to be ignored by the model's attention layers.\n",
    "    \n",
    "\n",
    "- **Padding**:\n",
    "  - Padding ensures all sequences in a batch match the length of the longest sequence by adding a special padding token (e.g., `[PAD]`).\n",
    "  - Padding is important for:\n",
    "    - Consistency in sequence length across a batch, ensuring efficient processing.\n",
    "    - Avoiding model bias caused by sequence length variations, which could affect performance.\n",
    "\n",
    "\n",
    "- **Example**:\n",
    "  - Sentences:\n",
    "    - \"I love NLP.\"\n",
    "    - \"Padding in tokenizers is useful.\"\n",
    "  - Maximum sequence length = 5 tokens.\n",
    "  - The shorter sentence is padded:\n",
    "    - \"I love NLP [PAD] [PAD]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c0d074-e5ec-4385-8594-6131ab8c74a3",
   "metadata": {},
   "source": [
    "## Tokenizers\n",
    "\n",
    "### Word-based Tokenizer\n",
    "\n",
    "  - Simple and easy to set up with few rules.\n",
    "\n",
    "\n",
    "  - Yields decent results for many applications.\n",
    "  \n",
    "  \n",
    "- **Goal:**\n",
    "  - Split raw text into words.\n",
    "  - Find a numerical representation for each word.\n",
    "\n",
    "\n",
    "- **Text Splitting Methods:**\n",
    "  - Can split text in different ways.\n",
    "  - Example: Using whitespace to tokenize text into words.\n",
    "  - Python's `split()` function can be used for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6c7d1d3-b396-4f38-bc4b-c57e7b9faec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenize',\n",
       " 'the',\n",
       " 'text',\n",
       " 'into',\n",
       " 'words',\n",
       " 'by',\n",
       " 'applying',\n",
       " 'Pythonâ€™s',\n",
       " 'split()',\n",
       " 'function']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = \"tokenize the text into words by applying Pythonâ€™s split() function\".split()\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fe4e2f-02fa-49ea-975f-baabe580b47a",
   "metadata": {},
   "source": [
    "- Word tokenizers can include extra rules for punctuation.\n",
    "\n",
    "\n",
    "- These tokenizers create vocabularies, defined by the total number of independent tokens in the corpus.\n",
    "\n",
    "\n",
    "- Each word in the corpus is assigned a unique ID, starting from 0, which the model uses to identify each word.\n",
    "\n",
    "\n",
    "- A comprehensive word-based tokenizer needs an identifier for every word in a language, leading to a large number of tokens.\n",
    "   - Example: The English language has over 500,000 words, meaning a vast vocabulary and many unique IDs.\n",
    "   - Words like \"dog\" and \"dogs\" or \"run\" and \"running\" are seen as unrelated by the model initially, as thereâ€™s no inherent recognition of similarity.\n",
    "\n",
    "\n",
    "- Tokenizers include an \"unknown\" token (often `[UNK]` or `<unk>`) for words not in the vocabulary.\n",
    "\n",
    " - If many unknown tokens are produced, it indicates that the tokenizer is struggling to represent words accurately, losing information.\n",
    "    \n",
    "###  Character-based\n",
    "\n",
    "**Character-based tokenization** splits text into characters rather than words.\n",
    "  \n",
    "  **Primary benefits:**\n",
    "  - Smaller vocabulary.\n",
    "  - Fewer out-of-vocabulary (unknown) tokens, as every word can be constructed from characters.\n",
    "  \n",
    "  \n",
    "  **Challenges:**\n",
    "  - Handling spaces and punctuation can raise issues.\n",
    "  \n",
    "  \n",
    "  **Drawbacks:**\n",
    "  - Representation may be less meaningful since individual characters carry less information compared to words.\n",
    "  - This varies across languages (e.g., Chinese characters hold more information than characters in Latin languages).\n",
    "  - It produces a larger number of tokens to process. A word that is a single token in word-based tokenization may become 10+ tokens in character-based tokenization.\n",
    "  \n",
    "\n",
    "### Subword tokenization\n",
    "\n",
    "**Subword tokenization** offers a compromise, combining word-based and character-based approaches.\n",
    "\n",
    "- Subword tokenization algorithms are based on the idea that:\n",
    "  - Frequently used words should not be split.\n",
    "  - Rare words should be decomposed into meaningful subwords.\n",
    "  \n",
    "  \n",
    "- Example: \n",
    "  - \"Annoyingly\" could be split into \"annoying\" and \"ly\".\n",
    "  - These subwords are more common and retain the original meaning.\n",
    "  - `Let's</w> do</w> token ization</w> !</w>`\n",
    "\n",
    "\n",
    "- Benefits of subword tokenization:\n",
    "  - Semantic meaning is preserved through subword combinations.\n",
    "  - Efficient representation of long words using fewer tokens.\n",
    "  - Achieves good vocabulary coverage.\n",
    "  - Minimizes unknown tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3210c4d-b531-4f39-b9d5-af566d419fef",
   "metadata": {},
   "source": [
    "### Loading and saving\n",
    "\n",
    "- Loading and saving tokenizers is similar to handling models.\n",
    "\n",
    "\n",
    "- It uses the same two methods: `from_pretrained()` and `save_pretrained()`.\n",
    "\n",
    "\n",
    "- These methods load or save:\n",
    "  - The algorithm used by the tokenizer (comparable to the architecture of a model).\n",
    "  - The vocabulary (comparable to the weights of a model).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d9c07e6-109d-403e-a54b-c8a70e2124b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/peltouz/Documents/pretrain/test/tokenizer_config.json',\n",
       " '/home/peltouz/Documents/pretrain/test/special_tokens_map.json',\n",
       " '/home/peltouz/Documents/pretrain/test/vocab.txt',\n",
       " '/home/peltouz/Documents/pretrain/test/added_tokens.json',\n",
       " '/home/peltouz/Documents/pretrain/test/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"/home/peltouz/Documents/pretrain/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ed691f-d523-4738-a261-07eefac94b6e",
   "metadata": {},
   "source": [
    "## Going through the model\n",
    "\n",
    "We can download our pretrained model the same way we did with our tokenizer. ðŸ¤— Transformers provides an AutoModel class which also has a `from_pretrained()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f15221b8-2de5-4be4-a71b-2f5ceafcea36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff1769-f1fd-4db4-b102-3f46e758af0f",
   "metadata": {},
   "source": [
    " - This code downloads and uses the same checkpoint as previously used in the pipeline.\n",
    " - Instantiates a model based on this checkpoint.\n",
    "\n",
    "\n",
    "- Architecture description:\n",
    "  - Contains only the base Transformer module.\n",
    "  - Takes inputs and outputs hidden states (also referred to as features).\n",
    "  - These hidden states represent a high-dimensional vector for each input, showing the contextual understanding of the Transformer model.\n",
    "\n",
    "\n",
    "\n",
    "- Use of hidden states:\n",
    "  - Hidden states are often inputs for another part of the model called the \"head.\"\n",
    "  - Different tasks, though using the same architecture, have different heads.\n",
    "  \n",
    "  \n",
    "\n",
    "- Output vector from the Transformer module:\n",
    "  - Typically, the vector has three dimensions:\n",
    "    1. **Batch size**: Number of sequences processed simultaneously (2 in this example).\n",
    "    2. **Sequence length**: Length of the sequence's numerical representation (5 in this example).\n",
    "    3. **Hidden size**: Vector dimension for each model input.\n",
    "    \n",
    "  - The high dimensionality comes from the hidden size (768 for smaller models, up to 3072 or more for larger models).\n",
    "\n",
    "\n",
    "\n",
    "We can see this if we feed the inputs we preprocessed to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0dbe9e7-b54b-4435-beb7-6c2616a3145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b42b3c5-1b42-4908-a09f-48d3b2efde45",
   "metadata": {},
   "source": [
    "- The outputs of ðŸ¤— Transformers models resemble namedtuples or dictionaries.\n",
    "\n",
    "\n",
    "- You can access elements in different ways:\n",
    "  - By attributes (e.g., `outputs.last_hidden_state`)\n",
    "  - By key (e.g., `outputs[\"last_hidden_state\"]`)\n",
    "  - By index, if you know the exact position (e.g., `outputs[0]`)\n",
    "\n",
    "## Model heads: Making sense out of numbers\n",
    "\n",
    "  - Take high-dimensional vectors of hidden states as input.\n",
    "  \n",
    "  - Project these onto a different dimension.\n",
    "  \n",
    "  - Usually composed of one or a few linear layers.\n",
    "\n",
    "\n",
    "- **Transformers and Model Heads:**\n",
    "  - The output of the Transformer model is sent directly to the model head for further processing.\n",
    "  - The embeddings layer in the model converts input IDs into vectors representing the associated tokens.\n",
    "  - Subsequent layers manipulate these vectors using the attention mechanism to produce final sentence representations.\n",
    "\n",
    "\n",
    "\n",
    "- **Available Architectures in ðŸ¤— Transformers:**\n",
    "  - Model (retrieves hidden states)\n",
    "  - ForCausalLM\n",
    "  - ForMaskedLM\n",
    "  - ForMultipleChoice\n",
    "  - ForQuestionAnswering\n",
    "  - ForSequenceClassification\n",
    "  - ForTokenClassification\n",
    "\n",
    "\n",
    "\n",
    " - For example for sentence classification tasks, use `AutoModelForSequenceClassification` instead of `AutoModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a149cab-6a1f-4ef9-bfae-95488473a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d939c7-aecd-4056-a97b-fc345ca518bf",
   "metadata": {},
   "source": [
    "- The model head reduces dimensionality by taking high-dimensional vectors as input.\n",
    "\n",
    "\n",
    "- It outputs vectors containing two values, one for each label.\n",
    "\n",
    "\n",
    "- Since there are two sentences and two labels, the resulting output has a shape of 2 x 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e184aaa2-61bb-46f0-b67e-50267a81d7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[ 3.7257, -3.0865],\n",
      "        [-4.1563,  4.4512]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits.shape)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be254cb-f1cd-405c-bec1-4b449750d1ef",
   "metadata": {},
   "source": [
    "- The values referred to are logits, not probabilities.\n",
    "\n",
    "\n",
    "- Logits are raw, unnormalized scores outputted by the model's last layer.\n",
    "\n",
    "\n",
    "- To convert logits to probabilities, they must pass through a SoftMax layer.\n",
    "  - SoftMax is a generalization of the logistic function to multiple dimensions.\n",
    "  - It is used in multinomial logistic regression.\n",
    "\n",
    "\n",
    "During training, the loss function typically combines the final activation function (e.g., SoftMax) with the loss function (e.g., cross entropy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc32927b-c000-4de9-acf4-882727db4f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9890e-01, 1.0991e-03],\n",
      "        [1.8271e-04, 9.9982e-01]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8f3e5-2187-4d65-8789-f6d73332fb53",
   "metadata": {},
   "source": [
    "- These are recognizable probability scores.\n",
    "\n",
    "- To get the labels corresponding to each position, we can inspect the id2label attribute of the model config (more on this in the next section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec1d1ea2-d6d6-4e63-bd11-61d428b688ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de505e17-8f8c-4320-827e-aa4fbe441279",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "- **AutoModel Class**: \n",
    "  - Designed to instantiate any model from a checkpoint.\n",
    "  - Functions as a wrapper for the various models in the library.\n",
    "  - Automatically guesses the appropriate model architecture for the checkpoint.\n",
    "  - Instantiates a model with the guessed architecture.\n",
    "\n",
    "\n",
    "- **Direct Model Class Usage**:\n",
    "  - If the model type is known, the specific class defining the architecture can be used directly (e.g., BERT model).\n",
    "\n",
    "\n",
    "## Creating a Transformer\n",
    "\n",
    "The first thing weâ€™ll need to do to initialize a BERT model is load a configuration object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5d4117c-ed20-483c-88c4-cbad08bc259a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertModel(config)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dcd38a-432e-4000-856a-e77f31a45bd3",
   "metadata": {},
   "source": [
    "- The model can be used in its current state but will output gibberish.\n",
    "\n",
    "\n",
    "- The model requires training before it can perform well.\n",
    "\n",
    "\n",
    "- Training the model from scratch would:\n",
    "  - Take a long time.\n",
    "  - Require a lot of data.\n",
    "  - Have a non-negligible environmental impact.\n",
    "\n",
    "\n",
    "- Instead, reusing pre-trained models can avoid unnecessary and duplicated efforts.\n",
    "\n",
    "\n",
    "- Pre-trained Transformer models can be easily loaded using the `from_pretrained()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6348f9bc-fe98-43eb-b6a5-9be1f6c8a563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c91b39138714d1cb76b487f4e036186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c90dd9edde4ec7ba200f94b7ae310a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9885fb9-52e3-43f2-aae7-973ae81bdf5b",
   "metadata": {},
   "source": [
    "- **Using AutoModel**:\n",
    "  - Replace `BertModel` with `AutoModel` to produce checkpoint-agnostic code.\n",
    "  - This ensures compatibility with different checkpoints trained for similar tasks, even if the architecture varies.\n",
    "\n",
    "\n",
    "- **Loading Pretrained Models**:\n",
    "  - In the example, `BertConfig` was not used; instead, a pretrained model (`bert-base-cased`) was loaded.\n",
    "  - This specific checkpoint was trained by the original BERT authors, with details available in the model card.\n",
    "\n",
    "\n",
    "- **Model Initialization and Usage**:\n",
    "  - The model is initialized with pretrained weights from the checkpoint.\n",
    "  - It can be used directly for inference or fine-tuned on new tasks.\n",
    "  - Using pretrained weights helps achieve good results faster than training from scratch.\n",
    "\n",
    "\n",
    "- **Caching and Customizing Cache Folder**:\n",
    "  - Weights are downloaded and cached in the default folder `~/.cache/huggingface/transformers`.\n",
    "  - Cache folder location can be customized by setting the `HF_HOME` environment variable.\n",
    "\n",
    "\n",
    "- **Model Identifiers**:\n",
    "  - The identifier used to load the model can be from any compatible model on the Model Hub.\n",
    "  - A full list of available BERT checkpoints can be found [here](https://huggingface.co/models?other=bert).\n",
    "\n",
    "\n",
    "## Saving methods\n",
    "\n",
    "- Use the `save_pretrained()` method to save a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "858ae4a0-d5e2-48e6-a212-3853876d46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/home/peltouz/Documents/pretrain/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6eb857-7c37-4225-8ca8-a26865537405",
   "metadata": {},
   "source": [
    "- **config.json file**:\n",
    "  - Contains attributes needed to build the model architecture.\n",
    "  - Includes metadata:\n",
    "    - Checkpoint origin.\n",
    "    - ðŸ¤— Transformers version used when the checkpoint was last saved.\n",
    "\n",
    "\n",
    "- **pytorch_model.bin file**:\n",
    "  - Known as the state dictionary.\n",
    "  - Contains all the modelâ€™s weights (parameters).\n",
    "\n",
    "\n",
    "- **Relationship**:\n",
    "  - The `config.json` file provides the model architecture.\n",
    "  - The `pytorch_model.bin` file holds the modelâ€™s parameters (weights).\n",
    "\n",
    "\n",
    "## Using a Transformer model for inference\n",
    "\n",
    "- **Making predictions with a model:**\n",
    "  - Transformer models process numbers generated by the tokenizer.\n",
    "  - Tokenizers cast inputs into the appropriate framework's tensors.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8649ca5d-7ac8-49cb-bc0f-716a108a1a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778fd314-9b55-44d8-9151-3d8152770090",
   "metadata": {},
   "source": [
    "- The tokenizer converts text into vocabulary indices.\n",
    "- These indices are typically referred to as input IDs.\n",
    "- Each sequence of text is transformed into a list of numbers.\n",
    "- The final result of the tokenizer is the output consisting of these numerical lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf491386-697a-431f-8b71-4724f45e1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f5870-2986-4ffc-be05-aa1a1698df5a",
   "metadata": {},
   "source": [
    "- Tensors require rectangular shapes, similar to matrices.\n",
    "\n",
    "\n",
    "- The provided \"array\" is already in a rectangular shape, making the conversion to a tensor straightforward.\n",
    "\n",
    "\n",
    "- Making use of the tensors with the model is extremely simple â€” we just call the model with the inputs.\n",
    "\n",
    "\n",
    "- Although the model can accept various arguments, only the input IDs are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f1d1ab6-d0d3-4949-b797-df19576efed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_inputs = torch.tensor(encoded_sequences)\n",
    "output = model(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aa3203-9191-440e-9395-4d7af676a0b0",
   "metadata": {},
   "source": [
    "#  Wrapping up: From tokenizer to model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bde613c3-42f2-4dad-b501-b6d7da5f2160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7b6fe5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [5.3534e-04, 9.9946e-01]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.nn.functional.softmax(output.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80fe92ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
